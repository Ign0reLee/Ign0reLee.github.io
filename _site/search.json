[
  
    {
      "title"       : "Attention And Transofrmer",
      "category"    : "",
      "tags"        : "Deep Learning, Machine Learning, Keras, Tensorflow, Attention, Transformer, PersonalStudy",
      "url"         : "./Attention-And-Transformer.html",
      "date"        : "2021-03-23 14:00:00 +0900",
      "description" : "",
      "content"     : "Attnetion Mechanism과 Trnasformer를 정리해보았습니다.학교 발표용 자료로, 자세한 내용은 생략하고, 간단하게 정리했습니다.참고 사이트1: Nvidia-DLI참고 사이트2: Wikidocs참고 사이트3: CS231N포함된 Microsoft Office 프레젠테이션, 제공: Office"
    } ,
  
    {
      "title"       : "Tensorflow with Keras",
      "category"    : "",
      "tags"        : "Deep Learning, Machine Learning, Keras, Tensorflow, Distributed Training, Sub-Classing, PersonalStudy",
      "url"         : "./Tensorflow-With-Keras.html",
      "date"        : "2020-12-18 22:00:00 +0900",
      "description" : "",
      "content"     : "Tensorflow2에서 Keras를 이용한 다양한 학습 방법들에 대해서 정리해보았습니다. Tensorflow2 Document를 따라 제작하였으며, 내용은 거의 같습니다.참고 사이트1참고 사이트2PPT의 코드포함된 Microsoft Office 프레젠테이션, 제공: Office"
    } ,
  
    {
      "title"       : "03. Data Science With RAPIDS Cupy Part 02",
      "category"    : "",
      "tags"        : "RAPIDS, Python, Machine Learning, DataScience, Cupy, cupy, Data, Data Science",
      "url"         : "./Data-Science-With-RAPIDS-Cupy-Part02.html",
      "date"        : "2020-08-19 22:00:00 +0900",
      "description" : "",
      "content"     : "안녕하세요, 두번째 챕터 Cupy Part 02입니다.오늘은 Cupy로 표현하는 선형대수학을 하도록 하겠습니다.마찬가지로 코드위주로 작성할 예정이기 때문에 원본코드를 먼저 첨부하겠습니다.원본코드 보러가기Cupy와 선형대수선형 대수는 데이터 과학에서 기술과 개념을 뒷받침 해주는 분야입니다.이번 챕터에서는 Cupy를 이용한 선형 대수 표현을 배우겠습니다.실은 Cupy에서 이미 대부분의 선형대수의 개념을 함수로 만들어 놓았습니다.그래도 아직 익수하지 못한 분들을 위해, 그리고 처음 보는 분들을 위해 선형 대수 부분을 넣어두었습니다.따라서 앞서 배운 기초 부분과 많이 유사할 예정이며, 사용에 익숙하신 분들이라면 넘어가셔도 관계 없습니다.자 그럼 시작하겠습니다.주피터 노트북 환경에서, 쉘별로 확인해보도록 하겠습니다.Import 하기 import cupy as cp import numpy as np이번 칼럼에선 항상 numpy와 cupy를 임포트 해옵니다.다만, 이번 챕터에서는 시간에 대한 부분은 빠져있습니다.함수 사용에 관해 배울땐, 시간을 고려할 필요가 없기 때문입니다.2.1 벡터2.1.0 벡터의 개념벡터와 스칼라는 수학을 공부해본 사람이라면 많이 보았을 개념일 것입니다.저는 수학과가 아니여서 그런지 벡터는 간단하게 방향이 있는 값 정도로 이해하고 있습니다.참고하고 있는 모 책에서는 어떤 유한한 차원의 공간에 존재하는 점들 이라고 설명합니다.대부분의 숫자는 벡터로 표현 가능합니다.그리고 앞으로 많은 경우 이 벡터로 계산, 표현 할 예정입니다.그러므로 벡터 사용에 익숙해 지는 것이 좋습니다.2.1.1 벡터의 표현벡터를 가장 간단하게 표현하는 방법은 Python의 list로 표현하는 방법입니다.하지만 저희는 Cupy와 Numpy로 이해할 예정이기 때문에 array로 표현할 예정입니다.list와 크게 다르지 않기 때문에 코드로 바로 확인해보겠습니다.numpy_vector = np.array([10, 20, 30])cupy_vector = cp.array([10, 20, 30])저희는 앞서 cupy의 기초 파트에서 cupy array의 사용법에 대해서 익혔습니다.기본적으론, numpy array와 같은 방법으로 사용하시면 됩니다.다만 cupy의 경우 데이터가 GPU 메모리에 올라갑니다.사진으로 확인해보겠습니다.2.1.2 벡터의 덧셈벡터의 덧셈에 대해 알아보겠습니다.벡터끼리 더한다는 것은, 각 벡터상에서 같은 위치에 있는 성분끼리 더한다는 의미입니다.Numpy와 Cupy 모두 add라는 함수로 구현되어 있습니다.그림으로 보았던 예시를 코드로 실험해보겠습니다.Numpyvector1 = np.array([1,2])vector2 = np.array([2,1])result = np.add(vector1, vector2)print(result)Cupyvector1 = cp.array([1,2])vector2 = cp.array([2,1])result = cp.add(vector1, vector2)print(result)2.1.3 벡터의 뺄셈벡터의 뺄셈도 덧셈과 마찬가지로 성분별로 진행합니다.Numpy와 Cupy 모두 subtract라는 함수로 구현되어 있습니다.Numpyvector1 = np.array([1,2])vector2 = np.array([2,1])result = np.subtract(vector1, vector2)print(result)Cupyvector1 = cp.array([1,2])vector2 = cp.array([2,1])result = cp.subtract(vector1, vector2)print(result)2.1.4 벡터와 스칼라곱벡터와 스칼라곱은 벡터의 각 원소에 스칼라를 곱하는 것으로 표현합니다.이는 multiply 함수로 표현할 수 있습니다.Numpyvector = np.array([1,2])scalar = 2result = np.multiply(vector, scalar)print(result)Cupyvector = cp.array([1,2])scalar = 2result = cp.multiply(vector, scalar)print(result)2.1.5 벡터의 내적백터의 내적은 각 요소별로 곱한 후 더한 값을 의미합니다.numpy와 cupy에선 dot이라는 함수로 구현되어 있습니다.Numpyvector1 = np.array([1,2])vector2 = np.array([2,1])result = np.dot(vector1, vector2)print(result)Cupyvector1 = cp.array([1,2])vector2 = cp.array([2,1])result = cp.dot(vector1, vector2)print(result)2.1.6 벡터의 요소별 제곱벡터의 요소별 제곱은 square라는 함수로 구현되어 있습니다.Numpyvector = np.array([1,2])result = np.square(vector)print(result)Cupyvector = cp.array([1,2])result = cp.square(vector)print(result)2.1.7 벡터의 요소별 루트벡터의 요소별 루트는 sqrt라는 함수로 구현되어 있습니다.Numpyvector = np.array([1,2])result = np.sqrt(vector)print(result)Cupyvector = cp.array([1,2])result = cp.sqrt(vector)print(result)2.1.8 벡터의 거리이제 앞서 배운 개념들을 이용해 간단하게 벡터간 거리 연산을 진행할 수 있습니다.거리에도 여러가지 개념이 존재하지만, 지금은 가장 일반적이라고 할 수 있는 유클리디언 거리를 측정해보도록 하겠습니다.Numpyvector1 = np.array([1,2])vector2 = np.array([2,3])sub = np.subtract(vector1, vector2)square = np.square(sub)sums = np.sum(square)result = np.sqrt(sums)print(result)Cupyvector1 = cp.array([1,2])vector2 = cp.array([2,3])sub = cp.subtract(vector1, vector2)square= cp.square(sub)dot = cp.dot(square, square)result = cp.sqrt(dot)print(result)2.1.9 벡터의 거리에 대한 다양한 표현그런데, 위의 표현은 뭔가 간결해보이지 않습니다.실은 같은 코드여도 여러가지 방식으로 표현할 수 있습니다.지금부터 좀 더 간결해지기 위해 여러가지로 코드를 바꿔 보겠습니다.당장 cupy를 사용하지 못하는 분들을 기준으로 하기 위해 numpy코드를 기준으로 해보겠습니다.vector1 = np.array([1,2])vector2 = np.array([2,3])result = np.sqrt(np.sum(np.square(np.subtract(vector1, vector2))))print(result)같은 코드인데, 한줄로 표현 가능합니다.뭐가 막 많아 보이지는 않지만 한 줄로 표현되어 있기에, 더 복잡해 보이기도 합니다.이를 해결하기 위해 square sum부분을 dot으로 바꿔보겠습니다.vector1 = np.array([1,2])vector2 = np.array([2,3])result = np.sqrt(np.dot(np.subtract(vector1, vector2), np.subtract(vector1, vector2)))print(result)조금 나아진 것 같기도한데, 같은 코드를 두번 실행 시켜야하는 점이 걸립니다.또한, 그 부분 때문에 더 복잡해 보이기도 합니다.따라서 이부분은 나눠서 표현하겠습니다.vector1 = np.array([1,2])vector2 = np.array([2,3])sub = np.subtract(vector1, vector2)result = np.sqrt(np.dot(sub, sub))print(result)조금 더 나아졌지만, 여전히 sub가 두번 들어가는 것이 뭔가 마음에 들지 않습니다.이를 함수로 표현하면 좀 더 나아질 수도 있습니다.def sum_of_square(v): return np.dot(v, v)def distance(v,w): return np.sqrt(sum_of_square(np.subtract(v,w)))vector1 = np.array([1,2])vector2 = np.array([2,3])result = distance(vector1, vector2)print(result)어떤가요? 조금 간결해졌나요?이 방법 외에도 다양하게 표현할 수 있지만, 우선은 이정도로 넘어가도록 하겠습니다.여러분들도 다양한 방법으로 거리를 표현해보시길 바라겠습니다.2.2 행렬2.2.0 행렬의 개념행렬은 보통 2차원 이상의 차원으로 구성되어 있는 숫자의 집합을 의미합니다.차원에 따라 2차원 행렬, 3차원 행렬, 다차원 행렬등으로 말하기도 합니다.행렬은 보통 list의 list등으로 표현합니다.즉, 2차원 list, 3차원 list등으로 작성함으로 표현할 수 있습니다.당연하겠지만, numpy와 cupy에선 array를 활용하여 표현할 수도 있습니다.2.2.1 행렬의 표현행렬 파트를 공부할 때는 random.rand함수를 사용하도록 하겠습니다.random은 행렬의 원소에 random한 실수 값을 주고 싶을 때 사용하는 메소드입니다.그 중 rand함수는, 원하는 shape의 랜덤한 array를 생성해줍니다.이때 값은 0에서 1사이입니다.NumpyA = np.random.rand(2,3)print(A)CupyB = cp.random.rand(2,3)print(B)2.2.2 행렬의 Shape행렬은 n개의 행과 k개의 열로 구성되어 있습니다.이 때, n과 k는 shape라는 함수로 구할 수 있습니다.NumpyA = np.random.rand(2,3)print(A)또한 numpy ndarray에 내장되어 있는 shape라는 함수를 사용해서도 구할 수 있습니다.print(A.shape)CupyB = cp.random.rand(2,3)print(B)cupy에서는 shape함수를 직접 제공하지 않습니다.cupy ndarray의 shape를 사용하여 구할 수 있습니다.2.2.3 단위 행렬기본적으로 단위 행렬이란, 대각선에 해당하는 성분은 1, 나머지는 0에 해당하는 n차 정사각 행렬을 의미합니다.numpy와 cupy에서는 eye라는 함수로 제공되어 있습니다.이 때, 굳이 정사각 행렬로 선언하지 않아도 관계 없습니다.단, 이런 경우 (0,0)지점 부터 대각 성분만 1이고, 나머지는 전부 0입니다.NumpyA = np.eye(3,3)print(A)CupyB = cp.eye(3,3)print(B)2.2.4 행렬 덧셈행렬 덧셈은 벡터간의 덧셈과 유사합니다.보통 요소별 덧셈을 의미합니다.또한 제공되는 함수도 같습니다.NumpyA = np.random.rand(2,3)B = np.random.rand(2,3)print(\"A\\n\",A)print(\"B\\n\",B)result = np.add(A,B)print(\"Result \\n\",result)CupyA = cp.random.rand(2,3)B = cp.random.rand(2,3)print(\"A\\n\",A)print(\"B\\n\",B)result = cp.add(A,B)print(\"Result \\n\",result)2.2.5 행렬 곱셈행렬 곱셈은 행렬간 이항 연산을 의미합니다.Numpy와 Cupy에서는 matmul이라는 함수로 제공합니다.NumpyA = np.random.rand(2,3)B = np.random.rand(3,2)print(\"A\\n\",A)print(\"B\\n\",B)result = np.matmul(A,B)print(\"Result \\n\",result)CupyA = cp.random.rand(2,3)B = cp.random.rand(3,2)print(\"A\\n\",A)print(\"B\\n\",B)result = cp.matmul(A,B)print(\"Result \\n\",result)마무리이것으로 cupy파트는 마치도록 하겠습니다.당연하겠지만 모든 부분을 설명하진 않았습니다.선형 대수학 부분도, cupy부분도 마찬가지입니다.더 배우고 싶으시다면, cupy나 numpy의 공식 document를 참고하시길 바라겠습니다.또한 질문 및 이야기는 언제든지 부탁드리겠습니다.질문은 제가 활동하고 있는 사이트 Devstu에서 부탁드리겠습니다.질문 하러 가기Numpy DocumentCupy Document"
    } ,
  
    {
      "title"       : "02. Data Science With RAPIDS Cupy Part 01",
      "category"    : "",
      "tags"        : "RAPIDS, Python, Machine Learning, DataScience, Cupy, cupy, Data, Data Science",
      "url"         : "./Data-Science-With-RAPIDS-Cupy-Part01.html",
      "date"        : "2020-08-09 22:00:00 +0900",
      "description" : "",
      "content"     : "안녕하세요, 두번째 챕터 Cupy Part 01입니다.오늘은 Cupy의 기초부터 시작하도록 하겠습니다.그리고 코드위주로 작성할 예정이기 때문에 원본코드를 먼저 첨부하겠습니다.원본코드 보러가기Cupy?Cupy란 무엇일까요?대부분의 사람들의 경우 python에서 수학적인 계산을 할 때 Numpy를 많이 사용할 것이라고 생각합니다.Numpy는 훌륭하고, 빠르고, 편하고, 좋은 라이브러리입니다.하지만 CPU에서 돌아간다는 점 때문에 대규모 작업을 처리할 때 작업시간이 조금 부담스러울 때가 있습니다.Cupy란, Python에서 NVIDIA CUDA를 사용한 가속화 컴퓨팅을 제공하는 오픈소스 라이브러리입니다.Cupy는 Numpy를 뛰어 넘는 속도를 보여주다고합니다.심지어, 자체 테스트에선 연산이 100배 이상 차이나는 경우도 있었다고 합니다.RAPIDS에서 제공하는 자체 Bechmark 기사를 같이 첨부하겠습니다.Single-GPU Cupy Speedups그렇다면 이런 Cupy는 어떻게 사용하는 걸까요?주피터 노트북 환경에서, 쉘별로 확인해보도록 하겠습니다.Import 하기 import cupy as cp import numpy as np이번 과정에서는 항상 Numpy와 Cupy 모두를 Import 할 예정입니다.실제 환경에서는 Numpy를 Cupy로 바꾸는 것 만으로 대부분의 코드가 포팅 가능하게 됩니다.왜냐하면 Numpy와 Cupy는 Method가 동일하기 때문입니다.하지만 실제론 조금 다른 부분도 있습니다. 중요한 부분은 이번 챕터에서 보고 넘어가도록 하겠습니다.Simple Test Code 간단한 코드를 통해 사용법을 보도록하겠습니다.먼저 Numpy 버전 코드입니다. %%time x_num = np.arange(6).reshape(2,3).astype(\"f\") print(\"X : \", x_num) print(\"X sum : \", x_num.sum(axis=1)) !nvidia-smi 다음은 Cupy 버전 코드입니다. %%time x_cp = cp.arange(6).reshape(2,3).astype('f') print(\"X : \" , x_cp) print(\"X sum : \", x_cp.sum(axis=1)) !nvidia-smi 지난번에 보았던 설치 부분에서도 봤던 코드와 유사합니다.결과 출력된 화면을 보면, GPU메모리가 올라간게 보이시나요?GPU에 Array가 올라갔음을 알 수 있습니다.Time 그런데 한가지 이상한 점이 있습니다.Numpy보다 빠르다고 이야기를 했는데, 위의 코드에서는 Numpy가 Cupy보다 동작 시간이 더 짧음을 볼 수 있습니다.어째서 Numpy의 동작 시간이 더 빨랐을까요?지금의 경우에는 연산량이 작기 때문입니다.무슨 이야기냐면, GPU는 항상 CPU보다 빠른것이 아닙니다.다음과 같은 상황에서는 CPU가 GPU보다 빠른 성능을 낼 수도 있습니다. 계산량이 충분하지 않은 경우 잘못된 구조로 GPU 아키텍쳐를 만들었을 경우 (쉘 동작 시간에서는)처음 호출하는 경우지금의 경우 3번과 1번에 해당하는 상황인 것 같습니다.자 그러면 실제로 연산량이 많을수록 Cupy가 동작시간이 더 짧은지 코드로 확인해보겠습니다.Computing Time Test지금 부터 간단하게 연산 시간 테스트를 해보겠습니다.랜덤하게 생성한 N * N 크기의 행렬을 두개 만든 후, 내적을 실행해보겠습니다.그리고 N의 크기를 증가 시켜가면서 속도를 테스트 해 볼 예정입니다.직접 해보셔도 좋고, 결과만 보고 가셔도 괜찮습니다.Case 1. n=100numpy %%time a = np.random.rand(n,n) b = np.random.rand(n,n) result = np.matmul(a,b) cupy %%time a = cp.random.rand(n,n) b = cp.random.rand(n,n) result = cp.matmul(a,b) Case 2. n=1000numpy %%time a = np.random.rand(n,n) b = np.random.rand(n,n) result = np.matmul(a,b) cupy %%time a = cp.random.rand(n,n) b = cp.random.rand(n,n) result = cp.matmul(a,b) Case 3. n=10000numpy %%time a = np.random.rand(n,n) b = np.random.rand(n,n) result = np.matmul(a,b) cupy %%time a = cp.random.rand(n,n) b = cp.random.rand(n,n) result = cp.matmul(a,b) 확실히 연산량이 늘어나면 늘어날수록 Numpy에 비해 Cupy가 훨씬 빠른 속도를 냄을 알 수 있습니다.반면 n이 작을땐, Numpy가 속도가 더 빠른걸 볼 수 있습니다.Cupy Data Type처음 Cupy 매서드와 Numpy의 매서드가 거의 동일하다고 말씀 드렸습니다.그렇다면 데이터 타입도 같을까요?한번 확인해보겠습니다. Num_array = np.arange(6) print(Num_array) print(type(Num_array)) Cupy_array = cp.arange(6) print(Cupy_array) print(type(Cupy_array)) 둘이 데이터 타입이 조금 다름을 알 수 있습니다.ndarray이긴 하나, Numpy의 ndarray와 cuda의 core에 올라가있는 cupy의 ndarray입니다..get()그렇다면, cupy ndarray를 numpy ndarray타입으로 바꿀 수는 없을까요?밑에 추가로 바꾸는 방법이 나오지만, 우선 .get()은 자주 사용할 예정임으로 따로 만들었습니다.cupy array뒤에 .get()을 붙임으로 numpy ndarray로 만들 수 있습니다. cpu_array = Cupy_array.get() print(cpu_array) print(type(cpu_array))Cupy Method마지막으로 앞으로 자주 쓰일 매서드를 몇가지만 살펴보도록 하겠습니다.Numpy 사용이 익숙하신 분들은 이 부분은 넘어가셔도 관계 없습니다.또한 제대로 이해하시려면 documentation을 참고하시기 바랍니다.지금은 쉽게 사용하기 위해 꼭 필요한 부분만 적어 넣었습니다.cupy documentation실습 코드와 결과는 원본 코드를 참고해주시길 바라겠습니다.cupy.array(arg)cupy ndarray를 반환합니다.cupy.arange(strat, stop=None, step=1, dtype=None)시작부터 끝지점까지 step 간격을 가진 cupy ndarray를 반환합니다.cupy.empty(shape, dtype)초기화한 cupy ndarray를 반환합니다.cupy.ones(shape, dtype)1로 초기화한 cupy ndarray를 반환합니다.cupy.zeros(shape, dtype)0으로 초기화한 cupy ndarray를 반환합니다.cupy.linalg.norm(cupy.ndarray)Euclidean norm(a.k.a L2 norm)한 결과를 반환합니다.cupy.cuda.Device(int).use()cupy는 기본적으로 gpu 0을 사용하게 되어있습니다. 이 명령어를 통해 원하는 gpu로 옮길 수 있습니다.cupy.asnumpy(cupy.ndarray)앞서 잠깐 살펴보았던 .get()메서드와 같은 역할입니다. 둘 중 원하는 방법으로 사용하셔도 무방합니다.cupy.add(array1, array2)두 어레이의 원소별 덧셈을 반환합니다.cupy.subtract(array1, array2)두 어레이의 원소별 뺄셈을 반환합니다.cupy.multiply(array1, array2)두 어레이의 원소별 곱셈을 반환합니다.cupy.divide(array1, array2)두 어레이의 원소별 나눗셈을 반환합니다.cupy.power(array1, array2)두 어레이의 원소별 승곱을 반환합니다.cupy.mod(array1, array2)두 어레이의 원소별 나머지를 반환합니다.cupy.absolute(array)어레이의 원소별 절대값을 취한 값을 반환합니다.cupy.exp(array)어레이의 원소별 Exponential 결과를 반환합니다.cupy.log(array)어레이의 원소별 log를 수행합니다.cupy.sqrt(array)어레이의 원소별 sqaure root 연산을 수행합니다.cupy.square(array)어레이의 원소별 제곱연산을 수행합니다.cupy.sin, cupy.cos, cupy.tan다양한 삼각함수들도 지원하고 있습니다.cupy.equal(array1, array2)두 어레이의 각 원소별로 값이 같은지 비교하여 반환합니다.cupy.maximum(array1, array2)두 어레이의 각 원소별로 큰 값을 반환합니다.cupy.minimum(array1, array2)두 어레이의 각 원소별로 작은 값을 반환합니다.cupy.floor(array)원소별 floor연산을 수행 후 반환합니다.cupy.ceil(array)원소별 ceil연산을 수행 후 반환합니다.마치며이것으로 오늘 과정을 마무리 하겠습니다.오늘은 기초 부분이라 명령어의 연속이었습니다.실은 cupy document를 참고 하셔도 무관합니다.다음에는 cupy를 이용한 선형대수 표현에 대해서 이야기해보겠습니다.질문 및 이야기는 언제든지 부탁드리겠습니다.질문은 제가 활동하고 있는 사이트 Devstu에서 부탁드리겠습니다.질문 하러 가기"
    } ,
  
    {
      "title"       : "01. RAPIDS Installation",
      "category"    : "",
      "tags"        : "RAPIDS, Python, Machine Learning, DataScience, Cupy, Cudf, CuML, Data, Data Science",
      "url"         : "./Data-Science-With-RAPIDS-Install.html",
      "date"        : "2020-08-02 22:00:00 +0900",
      "description" : "",
      "content"     : "우선 첫번째 챕터 설치부터 살펴보겠습니다.솔직히 설치 부분은 비교적 간단하기 때문에, 굳이 보시지 않으셔도 상관없습니다.다만, 설치시에 문제가 있으시던가 하는 것을 방지하기 위하여 그리고 RAPIDS공식 홈페이지에 대해 알려드리기 위하여, 작성하였습니다.Enviroment제가 사용한 환경은 다음과 같습니다. Ubuntu &gt;= 18.04 LSTAnacondaPython &gt;= 3.6.8RAPIDS(stable) &gt;= 0.12CUDA &gt;= 10.0Cudnn &gt;= 7.6NUMPY &gt;= 1.16.1여러분들은 각자가 원하는 환경으로 구성하시면 됩니다.다만, 이 칼럼에서는 저랑 비교적 똑같은 환경을 구축하시는것을 추천드립니다.RAPIDSRAPIDSRAPIDS에 대한 대부분의 자료는 이곳에서 찾아보실 수 있습니다.제가 작성하는 글은 이 사이트와, BLOG, SLACK, NVIDIA DLI등을 참고하여 작성하였습니다.Installation자, 그러면 설치를 시작해 볼까요?RAPIDS 공식 사이트에 GET STARTED 버튼을 누르면, 설치 방법과, 설치할 수 있는 환경등을 설명해줍니다.이번 칼럼에서는 아나콘다 환경에서의 설치 방법만 다루고 있습니다. 다른부분에 대해서는 다음 기회에 살펴보도록하겠습니다.아나콘다에서는 어떻게 설치하는지 그림으로 먼저 살펴보겠습니다.순서대로 살펴 볼까요?우선 가장 위에는 어떤 방법으로 설치할 것인지에 대한 부분입니다.지금은 콘다환경에서 사용하기 때문에 콘다를 선택했습니다.(실은 제가 도커를 다루줄 잘 모르기때문에, 도커에 대한 부분은 도커 칼럼을 연재하면서 배워두려고합니다.)그리고 Stable버젼과 Nightly버젼이 존재하는데, 우선 Stable버젼을 사용하도록하겠습니다.마찬가지로 전 우분투 18.04를 사용하였으나, 실은 16.04와 18.04가 설치시에는 명령어 차이가 존재하지 않습니다.그 다음은 파이썬 버젼에 대한 문제인데, 제 콘다 버젼은 파이썬 3.7까지 밖에 지원을 하지 않습니다.그리고, 제가 실은 텐서플로 1버젼을 즐겨 사용했었기에 3.6버젼으로 선택했습니다. 이는 취향의 문제임으로 본인 취향껏 선택하시길 바라겠습니다.마지막으로 CUDA버젼인데, 제가 사용하고 있는 노트북은 Geforce 1050을 사용하고 있습니다.따라서 지원 여부를 잘 보고 선택해야하는데, 지금은 그나마 가장 안정화 버젼은 CUDA10.0을 선택했습니다.(이 부분도, 어차피 콘다를 쓴다면 어떤 버젼을 사용하셔도 상관 없을 것으로 알고 있습니다.)자 이제 선택이 모두 끝났습니다.마지막으로 출력된 커맨드를 자신의 컴퓨터에 복사하여 사용하시면 됩니다.모두 귀찮으실수도 있기에, 제 글에도 제 환경과 동일한 명령어 부분만 작성해 놓겠습니다. conda install -c rapidsai -c nvidia -c conda-forge \\ -c defaults rapids=0.14 python=3.6 cudatoolkit=10.1Installation check자 이제 마지막으로 성공적으로 설치되었는지 확인해볼까요?간단한 명령어만 테스트 해보겠습니다.저는 주피터 환경에서 테스트 하였습니다. import cupy as cp import cudf as cd import cuml as cm X_cp = cp.array([1,2,3,4,5,6,7,8,9]) y_cp = cp.array([11,22,33,44,53,66,77,87,95]) print(\"X : \", X_cp) !nvidia-smi마지막으로 원본 코드 주소를 첨부하겠습니다.원본코드 보러가기깃허브에서 보시면 추가로 행렬간의 내적으로 시간을 측정한 부분도 포함되어있습니다.이번챕터에서는 다루지 않았지만, 시간이 더 빨랐다 정도만 챙겨가시면 될 것 같습니다.또한 질문 및 이야기는 언제든지 부탁드리겠습니다.질문은 제가 활동하고 있는 사이트 Devstu에서 부탁드리겠습니다.질문 하러 가기감사합니다! 다음주에 뵙겠습니다."
    } ,
  
    {
      "title"       : "00. Data Science With RAPIDS Introduction",
      "category"    : "",
      "tags"        : "RAPIDS, Python, Machine Learning, DataScience, Cupy, Cudf, CuML, Data, Data Science",
      "url"         : "./Data-Science-With-RAPIDS-Intro.html",
      "date"        : "2020-08-02 19:00:00 +0900",
      "description" : "",
      "content"     : "안녕하세요, 그리고 오랜만입니다.기나긴 준비 시간이 끝나고, 드디어 새로운 칼럼을 연재하고자합니다.과거 DLI Review를 진행하다가 RAPIDS와 처음 만났습니다.그리고 RAPIDS는 제게 굉장히 큰 충격을 안겨주었습니다.기존의 NUMPY와 PANDAS를 GPU에서 돌릴 수 있다.매서드가 동일하여 이름만 바꾸면 된다.이런 이야기들을 처음 그대로 들었을때는 별거 아니다 싶다가도 실제로 돌려보니 굉장히 개선되는 점이 많았습니다.아직 RAPIDS는 우리나라에 제대로 소개하는 글이 없어서 아쉬웠습니다.그래서 이번기회에 한글로 소개하고자합니다.아직 학부생이기에 부족한 점도 많고, 제가 모르는 점도 많을 것 같습니다.다만, GPU를 이용한 데이터 과학에 대해서 한번 생각해보시는 계기가 되셨으면합니다.Enviroment 아쉽지만, 이 칼럼에서는 환경을 조금 제한해야합니다.공식적으로 지원되고 있는 환경이 넓지는 않습니다.왠만하면 아래 환경을 따라가시기를 권장드립니다.또한 리눅스를 사용하기 어려우신 분들은 콜랩환경을 권장드립니다.다만, 콜랩 환경은 세션을 실행시킬 때 마다, RAPIDS를 새로 인스톨 해주어야합니다.이와 관련해서는 다시 글을 작성하도록 하겠습니다.Anaconda사용을 권장드립니다.마지막으로 Ubuntu환경을 작성하기로는 18.04LST 이상이라고 적혀있으나, 제가 사용하는 버전 이상이라는 의미입니다.또한, 하위 버전에서도 아마 가능할겁니다. 제가 아직 리눅스를 잘 다루지 못하여, 테스트를 할 수 없는점 죄송합니다. Ubuntu &gt;= 18.04 LSTAnacondaPython &gt;= 3.6.8RAPIDS(stable) &gt;= 0.12CUDA &gt;= 10.0Cudnn &gt;= 7.6NUMPY &gt;= 1.16.1Goal Of This Column 이 칼럼의 목표에 대해 알려드리겠습니다.우선 RAPIDS는 다음과 같이 구성되어 있습니다.이번 칼럼은 Data Science의 기초부분을 다루고자합니다.따라서 딥러닝 부분에 대한 이야기는 나중에 따로 칼럼을 준비하겠습니다.이 칼럼에서 알아보고자 하는 부분은 Numpy를 대체할 수 있는 Cupy,그리고 PANDAS를 대체할 수 있는 Cudf 마지막으로 scikit-learn을 대체할 수 있는 cuML부분입니다.또한 각각의 예시를 CPU버젼, GPU버젼으로 시간을 비교하며 진행하도록 하겠습니다. 앞으로 일주일에 한번 연재하는 것이 목표입니다.앞으로도 잘 부탁드리겠습니다. 감사합니다!질문 및 이야기는 언제든지 부탁드리겠습니다.질문은 제가 활동하고 있는 사이트 Devstu에서 부탁드리겠습니다.질문 하러 가기"
    } ,
  
    {
      "title"       : "Fundamentals Of Accelerated Data Science With RAPIDS",
      "category"    : "",
      "tags"        : "Nvidia Deep Learning Institute, Review, DLI, Data Science, RAPIDS, Python, Deep Learning, Machine Learning",
      "url"         : "./Nvidia-Accelerated-Data-Science-With-Rapids.html",
      "date"        : "2020-05-31 22:00:00 +0900",
      "description" : "",
      "content"     : "안녕하세요, NVIDIA DEEP LEARNING INSTITUTE REVIEW를 진행하고 있는 이제영입니다. 다섯번째 시간입니다! 이번엔 Fundamentals Of Accelerated Data Science With RAPIDS Review로 돌아왔습니다! 안타까운 소식이지만 아마 저희 스터디는 이번주로 마무리 지을 것 같습니다!, 기존에 친구들과 약속한 시간이 다 되었기 때문입니다. 아참 그리고 이번주는 인증서와 관련한 헤프닝이 있었습니다. 여러분들도 Accesment test시에 조심해주세요, 심사 숙고하신 후 제출해주시길 바라겠습니다! RAPIDS는 Pandas, Numpy등과 유사한 점이 많았습니다! 특히 Scikit-learn에 비해 편한 부분도 없잖아 있었습니다. 그래서 그런지 이번주는 그림이 조금 많네요! 블로그에 주기적으로 올리는 일은 앞으로 적어지겠지만 RAPIDS는 제가 개인적으로 마음에 들었기 때문에 종종 좋은 글이 보이면 해석해서 들고 오겠습니다! 여러분도 한번 RAPIDS를 시작해보는 것이 어떠신가요? 아무튼 이번주 리뷰 시작하도록하겠습니다!ReviewIndex -Summary-ExperienceSummaryIntroduction RAPIDSCourse Goals 일상적인 데이터 과학에 RAPIDS를 사용하는 핵심 도구에 대해 배우기 워크 스테이션 및 클러스터에서 클라우드 및 HPC까지 RAPIDS의 확장성 이해하기 계속해서 RAPIDS 기능을 배울 수 있는 기초 구축 &lt; 전통적인 모델의 GPU 작동 순서도 &gt;&lt; RAPIDS를 사용한 GPU 작동 순서도 &gt; RAPIDS는 전통적인 모델과는 다르게, CPU와 GPU의 사이의 복사 변환 과정이 별도로 필요하지 않습니다. 따라서 전통적인 모델에 비해서 더 빠른 성능을 낼 수 있습니다.&lt; RAPIDS의 구성 &gt; RAPIDS는 다음과 같이 구성되어 있습니다. 이번 과정에서 우리는 CuDF와, CuML, CuGRAPH, CUXFILTER를 살펴보았습니다. 대부분의 경우 기존의 다른 툴들과 거의 흡사하다는 느낌을 받았습니다.&lt; RAPIDS가 지원하는 데이터 과학용 툴 &gt; RAPIDS에서 사용할 수 있는 데이터 툴들이 어떤 툴과 유사한 기능을 하는지 한눈에 살펴볼 수 있습니다. 이번 스터디에서의 경험상 대부분의경우 CPU를 사용하는 기존의 툴들보다 RAPIDS가 훨씬더 빠른 성능을 내었습니다.&lt; RAPIDS의 공식 홈페이지 &gt; 저희가 이 과정에서 배우는 것은 극히 일부일 뿐이며, 더 자세한 내용의 위의 공식 홈페이지에서 확인할 수 있었습니다. [공식 홈페이지 바로가기](https://rapids.ai/) 처음 보시는 분이라면 다소 불편할 수도 있습니다. 하지만 천천히 보시면 금방 익숙해 지실 수 있습니다.&lt; CUDF의 설명 &gt; CuDF란, RAPIDS에서 Data Handling을 담당하는 모듈로서, GPU 가속화된 dataframes를 만들고 조작하도록 돕습니다. 또한 Pandas와 유사한 기능을 가지고 있습니다. &lt; RAPIDS로 연결할 수 있는 라이브러리들 &gt; RAPIDS로 핸들링한 데이터들 혹은 RAPIDS를 사용하여 다른 패키지들과 상호 보완적으로 사용할 수 있습니다. RAPIDS에서 공식적으로 지원하는 팩들과 인터페이스를 가지고 있는것들이 표시되어 있습니다.&lt; CuPY의 설명 &gt; CuPy역시 Data Handling 툴입니다만, 주로 계산의 영역을 담당합니다. CPU에서 Numpy와 같은 역할을 담당합니다. 사용 방법 역시 유사합니다.&lt; CUGRAPH의 설명 &gt; CuGRAPH는 Graph를 연구하기위한 NetworkX와 거의 유사합니다. NetworkX를 제가 사용해보지 않아서 용법이 유사한 것 까지는 모르겠습니다. 하지만 CUGRAPH를 활용하여 그래프를 분석하고, xFilter등을 활용하여 Visualize를 진행합니다. 그리고 다음의 용어를 배웠습니다. unique() – 값 Str.lstrip(‘#’) – 제거 Factorize() – 라벨링 Dask – 병렬 컴퓨팅&lt; RAPIDS에서 지원하는 TASK SCHEDULER 모형 &gt; 또한 다양한 Operation의 SCHEDULER를 제공합니다.&lt; CuML의 설명 &gt; 마지막으로 CuML입니다. 저는 개인적으로 Scikit-learn과 유사하다는 느낌을 많이 받았습니다. 이번 과정에서는 위에 보이는 5개의 알고리즘을 CuML를 사용하여 다루어 보았습니다. 솔직히 다루는 방법보다는 데이터가 어떻게 생겼는지를 더 많이 살펴본 것 같습니다. Experience Experience-이제영 RAPIDS는 굉장히 좋은 것 같다. 솔직히 Numpy, Numba, Pandas, Scikit-learn등을 따로 배우는 것 보다 편하고 좋은 것 같다. 과정 자체도 만족스럽다. 누구에게나 추천한다. 데이터 사이언스를 배우고 싶으면 비추천한다. RAPIDS의 Document가 생각보다 불편하게 되어 있는데, 이 과정을 수강하면 매우 도움이 될 것이다. 아직 RAPIDS가 완성이 아닌 것 같아 앞으로의 버전이 매우 기대가 된다. Experience-박경훈 CPU와 GPU의 속도차이를 결과로 확인할 수 있어서 좋았다. Pandas와 상당히 유사하기 때문에 접근하기 쉬웠다. 개인적으로 강의가 루즈해서 별로였다.Experience-구정수 GPU를 사용하여 데이터 분석을 빠르게 수행할 수 있었다. CPU만을 사용하는 경우와 비교해서 속도 차이가 컸다. Pandas, Numpy와 많은 부분에서 유사해서 기존의 데이터 과학을 배운 사람들이 손쉽게 사용할 수 있을 것이라 생각한다. Jupyter Lab 환경이 매우 편리했다.인증서 긴 글 읽어주셔서 감사합니다! 이번 과정은 배우는 내용보단 RAPIDS를 배운다는 느낌이 강합니다! 저는 매우 만족했던 과정이었던 것 같습니다. 특히 다들, Accesment에서 굉장히 헤매더라고요... 혹시 이 과정에 대해 관심 있으시면, 저희의 리뷰를 참고하시면 좋을 것 같습니다. 거듭 말씀드리지만 리뷰는 저희들의 개인적인 의견이며, 실제 과정에서는 위의 내용들과 더불어 코딩하시면서 배우실 수 있습니다! 한달동안 고생 많이 하셨습니다! 저희 DLI Review 스터디는 이로써 막을 내리지만, DLI 과정은 꾸준하게 열리고, 업데이트 되고 있습니다. 시작하기 앞서서, 어떤 과정인지 간단히 살펴보고 어떤느낌인가 정도를 보실때 참고가 되셨으면 좋겠습니다! 또한 궁금한점이 있으시면 언제든지 메일 바라겠습니다. 기다리고 있겠습니다. 마지막으로 홍보 하나만 하겠습니다! NVIDIA 는 AI 스타트업의 비즈니스/기술 개발에 필요한 여러 자원들을 지원해주는 인큐베이팅 프로그램을 운영하고 있고 전세계적으로 약 5,800 개의 스타트업들과 협력 노력을 하고 있습니다! 이 프로그램의 일환으로 실제 AI 기술을 적용하는 AI 스타트업에게 실질적인 지원을 해주고자 중소벤처기업부와 함께 N&amp;UP 프로그램을 진행한다고 합니다! NVIDIA 엔지니어 분들이 심사위원으로 참여하며 무려 사업 지원비 최대 3억, 뽑힌 30개의 기업들끼리 추가 심사를 통해 R&amp;D 투자비 최대 4억을 지원해주는 프로그램이라고 합니다! 자세한 내용은 링크 남기도록 하겠습니다![온라인 세미나](https://youtu.be/L717kWEmgbY)[모바일에서 자세히 보기](http://me2.do/FTnFsv66)[PC에서 자세히 보기](http://me2.do/FoAMy5jr) 감사합니다! 다음번에 다른 포스트로 다시 찾아뵙겠습니다!"
    } ,
  
    {
      "title"       : "Fundamentals of Accelerated Computing with CUDA Python Review",
      "category"    : "",
      "tags"        : "Nvidia Deep Learning Institute, Review, DLI, CUDA, Numba, Python, Deep Learning, Machine Learning",
      "url"         : "./Nvidia-DLI-CUDA-WITH-Python.html",
      "date"        : "2020-05-24 22:00:00 +0900",
      "description" : "",
      "content"     : "안녕하세요, NVIDIA DEEP LEARNING INSTITUTE REVIEW를 진행하고 있는 이제영입니다.네번째 시간입니다! 이번엔 Fundamentals of Accelerated Computing with CUDA Python Review로 돌아왔습니다!이번주는 그 전에 진행했던 Fundamentals of Accelerated Computing with CUDA C/C++ 과 비슷하다는 느낌을 많이 받았다고 하는데요내용 자체는 그 전과 크게 다르지 않으나 학기를 다니다보니 아무래도 조금 씩 늦어집니다.앞으로도 시간을 잘 지킬 수 있도록 노력하겠습니다!많이 부족하지만 앞으로도 잘 부탁드리겠습니다.질문있으시면 언제든지 메일을 보내주세요! 기다리고 있겠습니다.ReviewIndex -Summary-ExperienceSummaryIntroduction to CUDA Python with Numba이번 챕터의 목표 Numba를 활용하여 CPU에서 Python function들을 Compile 할 수 있습니다. Numba가 어떻게 Python Function들을 compile 하는지 알 수 있습니다. NumPy ufuncs GPU 가속화 할 수 있습니다. hand-written vectorized functions GPU 가속화 할 수 있습니다. CPU host와 GPU device간의 데이터 전송을 최적화 할 수 있습니다 Numba란 무엇인가? Numba는 CPU 또는 GPU환경에서 numerically-focused Python을 가속화 하기 위한 just-in-time, type-specializing, function compiler입니다. function compiler Numba는 파이썬 함수를 컴파일합니다. 어플리케이션 전체를 컴파일하지 않습니다. Numba는 파이썬 인터프리터를 대체하지 않습니다. 단지 기존의 함수를 더 빠른 함수로 대체하는 파이썬 모듈일 뿐입니다. type-specializing Numba는 특수한 데이터 타입에 대해 특별한 구현을 생성하면서 함수를 빠르게 합니다. 파이썬 함수는 보편적인 데이터 타입에서 작동하도록 설계되어있습니다. 이는 매우 유연하지만 느립니다. just-in-time Numba는 처음 선언되면 함수를 번역합니다. 어떤 변수 자료형이 사용될 지 알아야합니다. 기존 어플리케이션과 마찬가지로 Jupyter 노트북에서 대화식으로 사용할 수 있습니다. numerically-focused 일반적으로 Numba는 Numerical Data 타입에 맞춰져 있습니다. 그러므로 String형 데이터에 매우 제한적이고, 많은 String형 사용사례가 GPU에서 작동하지 않습니다. Numba에서 좋은 결과를 얻으려면 Numpy 배열을 사용해야합니다. How Numba Works&lt; Numba의 작동 순서도 &gt; Numba 컴파일러는 function decorator를 사용하여 활성화할 수 있습니다. Decorators는 함수를 변형합니다. Numba에서의 CPU compilation decorator는 @jit입니다..py_func attribute를 통해 original python function을 호출할 수 있습니다. @vectorize detorator를 사용하여 compiled ufuncs를 만들 수 있습니다. 특정 자료형을 지정하여 CUDA GPU에서 실행되도록 선언할 수 있습니다.GPU가 CPU보다 느린 경우, 아래의 상황인지 살펴봅시다.입력값이 너무 작은 경우계산이 너무 간단한 경우GPU로 데이터를 복사하거나 읽어들일 경우자료형이 필요 이상으로 클 경우CUDA Device Arrays를 사용하여 GPU 내에 데이터를 저장할 수 있습니다.Custom Kernels and Memory Management for CUDA Python with Numba이번 챕터의 목표 Python으로 커스텀 CUDA커널을 작성하고 실행합니다. Grid Stride Loops를 활용하여 대규모 DataSet에서 병렬로 작업하고 메모리를 통합 활용할 수 있습니다. atomic operations를 사용하여 병렬 작업시 race conditions를 회피합니다/ The Need for Custom Kernels GPU에서 가속화되는 ufuncs를 사용하는 것 보다 더욱 힘든 일입니다. custom CUDA kernels를 작성하는 것은 개발자에게 functions의 types에 대해 엄청난 유연성을 제공합니다.Introduction to CUDA Kernels CUDA로 프로그래밍 할 때 개발자는 병렬 커널 스레드의 GPU의 많은 코어에서 실행 되거나 CUDA 용어로 실행되는 커널이라는 GPU에 대한 함수를 작성합니다. 커널이 시작될 때 프로그래머는 \"Excution Configuration\"이라는 특수 구문을 사용하여 병렬 실행 구성을 설명합니다.CUDA Thread Hierarchy GPU에서 함수를 실행할 때 어떤 일이 일어나는지 알아봅시다. GPU는 병렬로 작업을 수행합니다. 이러한 작업은 thread에서 수행됩니다. 여러 threads가 병렬로 동작한다. CUDA는 수 천개의 threads를 병렬로 처리할 수 있습니다. threads의 집합을 block이라 한다. 여러 개의 block이 존재할 수 있습니다. 주어진 커널 실행에 대한 blocks의 집합을 grid라고 한다. GPU 함수를 kernel이라 부릅니다. kernels는 execution configuration과 함께 실행됩니다. execution configuration은 grid 안의 blocks의 수, 각 blocks의 threads의 수를 정의합니다. 한 grid 안의 모든 block은 같은 수의 threads를 가집니다. grid &gt; block &gt; threadCUDA-Provided Thread Hierarchy Variables gridDim.x 는 grid 안의 blocks의 수입니다. blockIdx.x 는 grid 안의 현재 block의 index입니다. blockDim.x 는 block 안의 threads의 수입니다. threadIdx.x 는 block 안의 thread의 index입니다.An Aside on Hiding Latency and Execution Configuration Choices CUDA 지원 NVIDIA GPU는 DRAM에 연결된 여러 개의 Streaming Multiprocessors 또는 SMs on a die로 구성됩니다. SM에는 많은 CUDA 코어를 포함한 커널 코드 실행에 필요한 모든 리소스가 포함되어 있습니다. 커널이 시작되면 각 블록은 단일 SM에 할당되고 잠재적으로 많은 블록이 단일 SM에 할당됩니다. SM은 블록을 \"워프 (warps)\"라고하는 32 개의 스레드로 세분화하고 실행하기 위해 병렬 명령이 제공되는 워프입니다. 따라서 GPU의 모든 잠재력을 활용하여 성능이 향상된 응용 프로그램을 작성하는 것이 가장 중요하기 때문에 SM이 커널을 실행하여 가장 간단하게 달성 할 수있는 충분한 수의 왜곡을 제공함으로써 대기 시간을 숨길 수있는 기능을 제공해야합니다. 그리드 및 블록 치수가 충분히 커야합니다.Atomic Operations and Avoiding Race Conditions다른 병렬 처리 프레임워크와 마찬가지로, CUDA 또한 race condition(경쟁 상태)가 발생할 수 있습니다 read-after-write hazards 한 스레드가 다른 스레드가 쓰는 동안 메모리 위치를 읽는 중입니다. write-after-write hazards 두 개의 스레드가 동일한 메모리 위치에 쓰고 있으며 커널이 완료되면 하나의 쓰기 만 표시됩니다. 이러한 문제를 피하려면 CUDA kernel algorithm을 잘 짜야 합니다. CUDA는 atomic operations를 제공합니다. 이를 활용하여 잘 피해갈 수 있습니다.Multidimensional Grids and Shared Memory for CUDA Python with Numba이번 챕터의 목표 다차원 블록 및 그리드를 사용하여 다차원 데이터 세트에서 GPU 가속 병렬 작업을 수행합니다. 공유 메모리를 사용하여 데이터를 칩에 캐시하고 느린 글로벌 메모리 액세스를 줄입니다. 2 and 3 Dimensional Blocks and Grids blocks = 4 =&gt; blocks = (2, 2) threads_per_block = 4 =&gt; threads_per_block = (2, 2)2 and 3 Dimensional Blocks and Grids grid = cuda.grid(1) =&gt; grid_y, grid_x = cuda.grid(2) stride= cuda.gridsize(1) =&gt; stride_y, stride_x = cuda.gridsize(2)Shared Memory Nunba는 blocks 간의 threads 사이의 shared memory를 할당할 수 있습니다. 병렬 threads 읽기나 쓰기에서 shared memory는 필수입니다. Shared memory를 선언할 때 shared memory의 크기를 설정해야 합니다. 이 크기는 정적인 값입니다. temp = cuda.shared.array(4, dtype=types.int32) idx = cuda.grid(1)Experience Experience-구정수 Experience-이제영NUMBA가 C++보다 편했습니다.다만 가독성이 C++보다 살짝 떨어지는 것 같다.총속도가 얼마나 빠르진 감이 안 잡힌다.C++은 못하겠고, CUDA는 해보고 싶으면 추천합니다.CUDA만 해보고싶으면 차라리 C++로 가는게 좋을 것 같다. Experience-박경훈NUMBA의 사용법을 간략하게 알 수 있다.C/C++ 가속화 컴퓨팅과 공통되는 부분이기 때문에 한 과정만 들어도 괜찮지만, C++ 과정이 좀 더 좋았다.시험이 어려웠다.Python을 다루는 분들에게 추천한다.Experience-구정수&lt;/center&gt;Python으로 GPU에서 동작하는 CUDA 명령어를 사용할 수 있다는 사실이 흥미로웠다.병렬 처리를 통해 기존의 연산보다 빠른 속도를 낼 수 있었다. 주어진 문제에 대한 설명이 좀 더 자세했으면 좋았을 것 같다. Python 문법을 잘 알고 있는 사람에게 추천할 만하다.인증서긴 글 읽어주셔서 감사합니다! C++때 겹치는 부분이 많아 block, thread등의 그림은 첨부하지 않았습니다!또 저희 친구들은 c++를 더 선호하는 경향이 있네요, 둘다 좋은 강의입니다!마찬가지로 모든 리뷰는 주관적이라는점 알아주시면 감사하겠습니다!앞으로 저희의 리뷰를 통해 Nvidia에서 열리는 DLI 프로그램에 대해 궁금하신점을 해결해가셨으면 좋겠습니다!이렇게만 보시면 감이 안잡히실 수도 있지만! 실제로 DLI프로그램을 진행하면 직접 코딩하시면서 하실 수 있습니다.이런 것들을 배우는 구나~ 가 더 중요하게 보셔야할 점인 것같습니다.다음주에는 Fundamentals of Accelerated Data Science with RAPIDS리뷰로 돌아오겠습니다! 감사합니다!"
    } ,
  
    {
      "title"       : "Image Super Resolution Using AutoEncoder Review",
      "category"    : "",
      "tags"        : "Nvidia Deep Learning Institute, Review, DLI, AutoEncoder, Image Super Resolution, Computer Vison, Deep Learning, Machine Learning",
      "url"         : "./Nvidia-Image-Super-Resolution-Using-AutoEncoder.html",
      "date"        : "2020-05-17 22:00:00 +0900",
      "description" : "",
      "content"     : "안녕하세요, NVIDIA DEEP LEARNING INSTITUTE REVIEW를 진행하고 있는 이제영입니다.세번째 시간입니다! 이번엔 Image Super Resolution Using AutoEncoder Review로 돌아왔습니다!이번주는 특이하게 인증서가 나오지 않았는데요, 반면 난이도는 꽤나 쉬운 편이었다고들 합니다!의외였던건 다른 스터디 인원들은 Auto Encoder를 기존에 이름만 듣고 직접 공부해본 적은 없다고 하던데, 구조 자체는 많이 봤었다고 하네요.Auto Encoder는 굉장히 중요한데, 이번 기회로 한번 다시 볼 수 있게되었습니다!많이 부족하지만 앞으로도 잘 부탁드리겠습니다.질문있으시면 언제든지 메일을 보내주세요! 기다리고 있겠습니다.ReviewIndex -Summary-ExperienceSummaryImage Super Resolution Using AutoEncoderSuper Resolution은 무엇일까요? Low Quality Image를 고품질 고해상도 이미지로 변환하는 것입니다.이미지의 해상도를 늘리려면(Upscaling), 커진 해상도에 따라 정보가 없는 픽셀은 어떻게 보간해야할까요? 전통적으로는 인접 픽셀의 값을 그대로 사용하였지만, Deep Learning을 이용하면 좀 더 자연스럽게 이미지를 Upscaling 할 수 있습니다.이번 챕터에서는 AI-based Super Resolution을 진행 해 볼 예정입니다.이는 기존의 Non-AI Methods들 보다 굉장히 높은 성능을 냅니다.Deep Learning이 Upscaling을 학습하려면 Encoder와 Decoder를 사용한 모델을 학습해야합니다.keras를 사용하여 Autoencoder를 학습해봅시다. 그리고 전통적인 Upscaling 방식과 비교해봅시다.IntroductionImage의 품질 하락에 영향을 주는 요소는 여러가지가 있지만, 이번 강의에서는 Low-Resolution Image에 대해 화질을 개선할 것입니다.실제로 천문학이나 단층 촬영과 같은 많은 분야에서 획득된 이미지는 유물과 소음을 포함하고 종종 낮은 해상도를 가집니다.이러한 감쇠는 센서의 제한에서 비롯되는 경우가 많습니다.&lt; 제한된 예시 &gt;How to improve a low quality image AI-based methods들이 성능이 좋은 이유는 일반화를 잘하기 때문입니다. 이번 챕터에서는 AutoEncoder를 사용합니다. AutoEncoder는 Artificial Neural Networks의 한 종류입니다.Autoencoders AutoEncoder란 무엇일까요? 그림을 먼저 보겠습니다. &lt; 일반적인 AutoEncoder &gt; Autoencoders는 AI의 “identity functions”입니다. 데이터를 입력받으며 무언가를 수행합니다. 그리고 원본의 입력을 다시 반환합니다. 더 작은 feature space를 위해 데이터를 압축한다는 것이 autoencoders가 압축에 유용하다는 것을 의미하진 않습니다. Autoencoders는 입력 데이터를 다른 space로 encode하는데, 이는 어쨌든 손실이 일어난다는 의미입니다. What makes up an autoencoder?Autoencoder는 아래와 같이 구성됩니다. : Encoder 보통 클래식한 뉴럴 네트워크 입니다. Input의 차원을 줄입니다. Decoder 보통 클래식한 뉴럴 네트워크 입니다. 새로운 공간에 이미지를 복원하려고 합니다. Loss Functions 입력과 출력 사이의 차이(혹은 거리)를 묘사하는 방법입니다. What kind of data does an Autoencoder manipulate Autoencoder는 손실 압축이지만 우리는 이점을 얻습니다. 이번 강의에서 예를 들면, 우리는 low quality image information에 대해 손실을 가지며, 새로운 선명한 image로 재건하는 것입니다.어떻게 autoencoder가 위의 손실을 학습할까? Autoencoder는 다양한 이미지들의 한 쌍에 대해서 패턴을 찾아야합니다. 어떻게 autoencoder가 위의 손실을 학습할까? Autoencoder는 다양한 이미지들의 한 쌍에 대해서 패턴을 찾아야합니다. 지금 부터 우리가 사용할 모델과, 그 결과를 살펴 보겠습니다! 사용된 코드와 설명은 직접 DLI를 수강하셔서 보시길 바랍니다! 이번엔 이런 구조의 아키텍쳐를 사용했고, 이런결과가 나왔음을 그림으로 보여드리겠습니다!The EncoderThe DecoderThe ModelDisplay the resultsHow to measure image quality두 이미지의 시각적 질의 차이를 논하기 위해, loss function을 정의해야합니다. 이번 section에서는 두 이미지 사이의 차이에 대한 loss function에 대해 살펴봅니다.MSEMean Squared Error는 값이 0이라면 차이가 없음을 나타냅니다. MSE는 이미지의 밝기값에 민감하다는 단점이 있지만 high_res 와 low_res 이미지를 비교하기엔 충분합니다.SSIM Structural similarity(SSIM)는 TV 또는 그러한 미디어에서 다루어질 두 이미지 사이의 유사도를 측정합니다. SSIM은 [1, -1]사이의 값을 가집니다. 여기서 1은 두 이미지가 유사함을 의미하고 값이 낮을수록 큰 차이가 있다는 것을 알려줍니다. SSIM는 이미지 안의 작은 영역을 정해서 두 이미지를 비교합니다. PSNR Peak signal-to-noise ratio는 MSE를 이용하여 정의된 metric입니다. PSNR은 reconstruction의 lossy compression의 질을 측정하기 위해 대개 사용됩니다. Low resolution과 pixelization은 압축의 한 형태로 여겨질 수 있습니다. 노이즈가 없다면, PSNR은 무한대의 값을 가진다. 그래서 우리는 PSNR을 최대화 해야합니다.HFENN HFENNHigh Frequency Error Norm Normalized) metric은 두 이미지 사이의 고주파 디테일이 차이가 나는지 측정합니다. 이것은 이미지가 더 많은, 혹은 더 적은 high frequency details을 지니는지 추정할 수 있게 만듭니다. 출력 값이 0이면 서로 같습니다. 값이 크면, 두 이미지 사이에 perceptual difference가 존재합니다.What to remember AutoEncoder는 Deep Neural Networks입니다. Autoencoder는 데이터를 작은 공간으로 인코딩합니다. Autoencoder는 두 이미지 사이에서 인코딩 할 표현의 패턴을 찾습니다. 네트워크 아키텍쳐는 경험적으로 만듭니다. Loss와 Metrics들은 학습에 굉장히 중요한 요소로 사용됩니다.Experience Experience-구정수Autoencoder와 encoder, decoder에 대한 자세한 설명이 인상적이었다.convolution layer와 upsampling, downsampling, dropout, merge layer에 대해 개념적으로 잘 설명되어 있어서 쉽게 이해할 수 있었다.중간 과정에서 데이터를 시각화하여 볼 수 있어서 더욱 쉽게 이해되었다.두 이미지를 비교하는 loss metrics를 여러가지 소개해주어서 새롭게 배웠다.CNN을 막 배우고 어떻게 활용되는지 알아보고 싶은 학생들에게 추천하고 싶다. Experience-이제영Auto Encoder의 중요한 부분만 잘 찝어낸 것 같다.케라스로 표현하여 쉽게 표현했다.다양한 부가 기법들을 소개해주었다.처음 시작하는 단계 사람들에게 추천할만 한 것 같다.내용이 많지는 않았다. Experience-박경훈짧은 강의이기 때문에 깊은 내용은 알 수 없다.해상도를 높이는 과정을 간략하게 알 수 있다.간단한 예제를 하기 때문에 크게 다가오지 않는다.기본적으로 딥러닝에 대한 기초가 있는 사람이 수강하는걸 추천한다.긴 글 읽어주셔서 감사합니다! 내용이 적다고 생각했는데, 다들 자세하게 다뤄주어서 길어졌습니다!마찬가지로 모든 리뷰는 주관적이라는점 알아주시면 감사하겠습니다!앞으로 저희의 리뷰를 통해 Nvidia에서 열리는 DLI 프로그램에 대해 궁금하신점을 해결해가셨으면 좋겠습니다!리뷰는 내용 요약이 들어가있지만, 실제로 DLI프로그램을 진행하면 직접 코딩하시면서 하실 수 있습니다.언제나 처럼 이런 흐름이다~ 라고 생각하시고 보시면 좋을 것 같습니다, 감사합니다!다음주에는 Fundamentals of Accelerated Computing with CUDA Python리뷰로 돌아오겠습니다!"
    } ,
  
    {
      "title"       : "Fundamentals of Accelerated Computing with CUDA C/C++ Review",
      "category"    : "",
      "tags"        : "Nvidia Deep Learning Institute, Review, DLI, CUDA, NVCC, C/C++, Deep Learning, Machine Learning",
      "url"         : "./Nvidia-DLI-CUDA-WITH-C-C++.html",
      "date"        : "2020-05-10 22:00:00 +0900",
      "description" : "",
      "content"     : "안녕하세요, NVIDIA DEEP LEARNING INSTITUTE REVIEW를 진행하고 있는 이제영입니다.드디어 두번째 시간이 다가왔습니다! 스터디를 이어가게 해주신 손해인님, Nvidia-Korea분들 모두에게 감사드립니다!이번주는 저번주에 말씀드렸듯이 ACCELERATED COMPUTING WITH CUDA C/C++을 진행하였습니다.기존에 CUDA 코딩과 관련하여 공부해보고 싶었지만 NVCC Document는 솔직히 저같은 초보자가 보기에는 조금 어려운 점이 많았습니다.이번 스터디를 통해 관련한 내용을 조금이라도 배울 수 있어서 매우 의미 있는 시간이었습니다. 감사합니다!많이 부족하지만 앞으로도 잘 부탁드리겠습니다.질문있으시면 언제든지 메일을 보내주세요! 기다리고 있겠습니다.ReviewIndex -Summary-ExperienceSummaryAccelerating Applications with CUDA C/C++CUDA는 세계에서 가장 성능이 뛰어난 병렬 프로세서인 NVIDA GPU에서 가속화되고 대량 병렬화된 코드를 실행할 수 있도록 해준다. 따라서 CUDA를 배운다면 CPU로는 불가능한 계산을 가능하게 해줍니다.__global__ void GPUFunciont() : __global__ 키워드는 뒤에 함수가 GPU에서 실행된다는 것을 말합니다.GPUFunction«&lt;a, b, c, d, e»&gt;() : GPU함수를 실행시킬 때 “«&lt; … »&gt;” 이런 기호를 쓰는데 a에는 블록 수, b에는 스레드 수, c에는 바이트 수, d에는 스트림, e에는 뭐라한다. 우린 이런 함수를 커널이라고 부릅니다.cudaDeviceSynchronize() : 실행 커널은 비동기적이기 때문에 CPU 코드는 커널의 끝을 기다리지 않는다. 따라서 이 코드를 호출한다면 커널이 완료될 때까지 CPU코드를 기다리게 할 수 있습니다.Writing Application Code for the GPUcudaDeviceSynchronize() 이 코드를 어디에서 호출하냐에 따라 결과가 달라집니다.Launching Parallel Kernels&lt;&lt;&lt; NUMBER_OF_BLOCKS, NUMBER_OF_THREADS_PER_BLOCKS&gt;&gt;&gt;grdiDim.x = NOB, blockDim.x = NOT, blockIdx.x = 특정 블록, threadIdx.x = 특정 스레드커널을 이용해 다음과 같이 바꿔줄 수 있습니다.Allocating Memory to be accessed on the GPU and the CPUMalloc() 함수로 메모리를 할당 받은 포인터는 GPU 코드를 참조 할 수 없습니다. GPU 코드를 실행하기 위해서는 cudaMallocManaged() 함수를 사용해야 하며 free() 함수 말고 cudaFree() 함수를 통해 해제를 해줘야한다.Accelerated Computing SummaryAt this point in time you have accomplished all of the following lab objectives: C/C++을 이용하여 CPU functions와 GPU kernel 연결하는 프로그램을 쓰고, 컴파일하고, 돌려보았습니다. Execution Configuration을 사용하여 병렬 스레드 계층 제어를 해보았습니다. Loop 문을 리팩토링하여 GPU에서 병렬로 실행했습니다. CPU와 GPU에 메모리를 할당하고, 비우고를 해보았습니다. CUDA 코드를 이용한 Error Handling을 해보았습니다. Now you will complete the final objective of the lab: CPU-only applications을 가속화 하였습니다. Iterative Optimizations with the NVIDIA Command Line ProfilerNsys는 Nsight Systems 명령어 도구입니다. 응용 프로그램의 GPU 활동 요약, CUDA API 호출 및 Unified Memory 활동에 대한 정보를 출력해줍니다.Asynchronous Memory Prefetching호스트나 디바이스가 메모리에 접근하려고 하면 Page fault가 발생하여 필요한 데이터를 일괄적으로 이동합니다. 이런 Page fault와 요구로 인한 메모리의 이동의 오버헤드를 줄이기 위한 기술을 비동기 메모리 프리패치라고 합니다. 코드를 사용하기 전에 통합메모리를 비동기적으로 이동시켜 GPU 커널과 CPU 성능을 높일 수 있습니다.Managing Accelerated Application Memory with CUDA Unified Memory and nsys SummaryAt this point in the lab, you are able to: Nsight Systems 명령 줄 도구 (nsys)를 사용하여 가속화 된 응용 프로그램 성능을 profile할 수 있었습니다.. \"Streaming MultiProcessors\"에 대한 이해를 활용하여 실행 구성을 최적화 해보았습니다. Page Faulting 및 Data Migrations과 관련하여 \"Unified Memory\"의 동작을 이해했습니다. 성능 향상을 도모한 Page Faulting및 Data Migrations을 줄이기 위해 \"Asynchronous Memory Prefetching\"을 사용해보았습니다. Iterative Development Cycle를 사용하여 애플리케이션을 빠르게 가속화하고 배포해보았습니다. 학습을 통합하고 애플리케이션을 반복적으로 가속화, 최적화 및 배포 할 수있는 능력을 강화하기위해 실습의 최종 실습을 진행했습니다. 완료 한 후 시간과 관심이있는 사람들을 위해 만들어진 \"고급 컨텐츠\"섹션을 진행했습니다. Iterative Optimizations with the NVIDIA Command Line ProfilerNsys는 Nsight Systems 명령어 도구다. 응용 프로그램의 GPU 활동 요약, CUDA API 호출 및 Unified Memory 활동에 대한 정보를 출력해줍니다.Concurrent CUDA StreamsCUDA 프로그래밍에서 스트림은 순서대로 실행되는 명령이 연속적으로 있습니다.하지만 기본 스트림 외에 다른 스트림을 생성하여 활용한다면 다중 작업을 서로 다른 스트림에서 동시에 수행할 수 있습니다. 기본 스트림이 아닌 다른 스트림에서 커널을 실행하기 위해서는 커널의 4번째 인수를 사용해야 합니다.Manual Device Memory Allocation and CopyingcudaMallocManaged()와 cudaMemPrefetchAsync()도 충분히 성능이 좋지만 좀 더 좋게하는 방법이 있습니다.cudaMalloc()은 메모리를 GPU에 직접 할당하지만 반환되는 포인터는 호스트 코드로 접근이 불가능합니다.cudaMallocHost()는 CPU에 직접 할당하고 마찬가지로 디바이스 코드는 접근이 불가능합니다.cudaMemcpy()를 통해 호스트와 디바이스간에 메모리를 복사해서 사용할 수 있습니다.마무리로 고정된 메모리는 cudaFreeHost()와 cudaFree()로 해제하면 됩니다.Asynchronous Streaming, and Visual Profiling for Accelerated Applications with CUDA C/C++ SummaryAt this point in the lab, you are able to: \"Nsight Systems\"를 사용하여 GPU 가속 CUDA 애플리케이션의 타임 라인을 시각적으로 Profile했습니다. Nsight Systems를 사용하여 GPU 가속 CUDA 애플리케이션에서 최적화 기회를 식별하고 활용해 보았습니다. 가속화 된 애플리케이션에서 동시 커널 실행을 위해 CUDA 스트림을 활용 해보았습니다. 마지막 연습에서 신체-시뮬레이터를 가속화하기 위해 배운 모든 것을 적용 할 수 있었습니다. Experience Experience-이제영솔직히 nvcc 도큐멘트 보다 잘 만든것 같다.NVCC를 너무 쉽게 잘 설명해주었다.Nsight을 왜 설치 하는지 드디어 알게 되었다.C/C++의 이해가 조금 부족해도 잘 할 수 있었다.NVCC를 처음 접하는 사람에게 추천합니다.NVIDIA CUDA를 이해하고 싶은 사람에게 추천합니다.분산 컴퓨팅에 대해 이해하고 싶은 사람에게 추천합니다. Experience-박경훈GPU와 CPU를 다룰 수 있어 흥미로웠다.Nsight System을 통해 시각적으로 볼 수 있어 재미있다.단계 별로 비교하는 문제에서 좀 더 구체적인 솔루션이 있었으면 좋겠다GPU에 관심이 있는 사람에게 추천해주고 싶다.인증서긴 글 읽어주셔서 감사합니다! 이번주는 내용이 많으면서도 적어서 최대한 흐름만 적기위해 노력하다보니 더 길어졌습니다!마찬가지로 모든 리뷰는 주관적이라는점 알아주시면 감사하겠습니다!앞으로 저희의 리뷰를 통해 Nvidia에서 열리는 DLI 프로그램에 대해 궁금하신점을 해결해가셨으면 좋겠습니다!지금 리뷰는 내용 요약이 들어가있지만, 실제로 DLI프로그램을 진행하면 직접 코딩하시면서 하실 수 있습니다.이런 흐름이다~ 라고 생각하시고 보시면 좋을 것 같습니다 ㅎㅎ 감사합니다!다음주에는 Image Super Resolution Using Autoencoders리뷰로 돌아오겠습니다!"
    } ,
  
    {
      "title"       : "Fundamentals of DL for Computer Vision Review",
      "category"    : "",
      "tags"        : "Nvidia Deep Learning Institute, Review, DLI, Computer Vision, Deep Learning, Machine Learning",
      "url"         : "./Nvidia-DLI-Computer-Vision.html",
      "date"        : "2020-05-03 22:00:00 +0900",
      "description" : "",
      "content"     : "우연치 않은 기회로 DLI리뷰 스터디들 진행하게되었습니다.각자 다른 이유로 모인 세명이지만 성실이 리뷰를 진행해 볼까합니다.처음 진행하는 리뷰이기에 어떻게 진행하는지 잘 몰랐던 것이 가장 힘든 점이었습니다.저희는 매주 밤 10시에 모여 PPT를 통해 요약, 느낀점, 추천여부등을 발표하는 활동을 진행하고 있습니다.총 요약본은 그림으로 같이 첨부하도록 하겠습니다.많이 부족하지만 앞으로도 잘 부탁드리겠습니다.질문있으시면 언제든지 메일을 보내주세요! 기다리고 있겠습니다.또한 이런 기회를 만들어주신 Nvidia, 손해인님께 감사드립니다.ReviewIndex -Summary-Experience-RecommendationSummaryWelcomeIntroduction of Deep LearningBrief the objectives of this course.Training Deep Neural NetworksBiological Inspiration 딥러닝은 컴퓨터가 examples(data)로부터 학습하게 만든다. 이 때, 두가지를 고려해야한다. 문제속 패턴이 식별되는가? 패턴을 보여줄 만큼 데이터가 충분한가? Game 1 아무런 사전 정보 없이 사진 속에 Louie가 있는지 확신에 대해 0~10사이의 점수를 매긴다. Deep Neural Networks : GPU Task1 영상 : Sweeping across industries 딥러닝이 산업 어느 분야에서 쓰이는지 살펴본다. 과거의 전통적인 기계학습과 Deep/End-to-End Learning의 차이 기계학습은 Hand designed feature가 제공되지만 Deep Learning은 그렇지 않다. End-to-End의 의미 : 문제 식별을 위한 패턴을 스스로 학습한다. DIGITS를 이용한 Image classifier 구현 사진이 Louie인지 아닌지 판별하는 모델을 만들어본다. 2 Epochs만 학습했을 경우, 정확도가 50% 이하지만 100 Epochs동안 학습한 후, 정확도가 99%에 이른다. 영상 모델의 output과 실제 output을 비교하고 차이를 이용하여 Weights를 갱신한다. Big Data : GPU Task 2 근래 기계학습을 발전시키는데 기여한 요소 세 가지가 있다. Deep Neural Network The GPU Big data 이 세 가지를 neural network를 학습시키는데 사용해볼 것이다. 새로운 데이터에 대해 neural network가 잘 작동하도록 학습시키는 것은 환경의 다양성이 표현되는 충분한 데이터가 필요하다. 영상 Deploying out Model : GPU Task 3 개와 고양이 구분하기. DIGITS Examples&lt;&lt;1. Digits Data Load Example&gt;&gt;&lt;&lt;2. Digits Train model 1&gt;&gt;&lt;&lt;3. Digits Train model 1&gt;&gt; Performance Performance during Training : GPU Task 4 정확도를 상승시키는 방법에 대해 공부한다. Object Detection : GPU Task5 Using deployment Caffe를 이용하여 객체 인식을 학습해보자. 의의 : Deep Learning과 전통 프로그래밍을 합쳐 이전의 불가능한 문제들에 대해 높은 성능을 보여줌을 설명한다. Assessment Train and deploy a deep neural network. 고래의 머리가 그려져 있는 사진과, 그렇지 않은 사진들로 이루어진 데이터셋을 학습시켜라. DIGITS와 Caffe(Python) 사용법을 테스트한다. Model과 Weights, Dataset 경로 설정 관련 빈칸 문제가 나온다. 최종적으로 모델을 학습시키는 메서드의 사용법을 숙지 해야 한다. ExperienceExperience-구정수기본적으로 영어에 대한 거부감이 없으며, 영어로 된 교육자료를 읽고 이해할 수 있는 능력이 필요하다.Deep Learning의 기초 알고리즘에 대해 밑바닥부터 자세하게 알려주진 않는다. 딥러닝의 커다란 개념들을 설명한다.학습 과정을 DIGITS 프레임워크를 통해 이해하기 쉽게 시각화하여 볼 수 있다.쉽게 적용 가능하고 성능이 좋은 딥러닝 모델과 데이터셋이 준비되어있다. DIGITS를 사용하여 공부하는 것은 장점과 단점 모두 가지고 있다.장점은 어렵지 않게 학습 과정을 살펴볼 수 있다는 것이고단점은 학습에 사용되는 알고리즘을 공부할 수 없다는 것이다. Experience-이제영Digits을 사용함으로써 모델이 더 간단 해졌다.기본이 caffe에서 추가된 부분인 것 같던데, 다루기 힘든 부분을 다루기 쉽도록 잘 도와준 것 같다.그냥 내용에서 직접 하는 부분이 proto 부분등 이므로 조금 부족한 것 같기도 하다.처음 배우는 사람이라면 굉장히 알차고 눈에 보이는 성과를 금방 낼 수 있을 것 같아 즐거울 것 같다. Experience-박경훈강의를 시작하게 되면 어떤 순서대로 강의를 진행하는지 한눈에 알 수 있다. 그 강의 시간도 표기가 되어있기 때문에 시간관리에 용이했다. 중간에 강의를 그만두어도 진행한 부분으로 바로 이동하여 편리했다. 강의 내용은 글, 동영상, 퀴즈, 실습 으로 구성 되어있다. 내용은 굉장히 이해하기 쉽게 설명 되어있고, 실습과 퀴즈를 통해 빠르게 이해할 수 있었다.실습은 NVIDIA에서 제공하는 서버를 이용하기 때문에 컴퓨터에 대한 걱정은 하지 않았다.실습을 진행하는 방법을 잘 강의해주기 때문에 진행하는데 있어서 문제는 없었다. 제공 받은 데이터를 이용해 직접 모델을 만들 수 있고, 그 모델을 바로 테스트 해볼 수 있기 때문에 굉장히 재미 있었다. Recommendation Experience-구정수시간이 여유롭고 영어와 친해지고 싶다, 혹은 영어에 부담 없는 사람들에겐 추천할 만 하다.딥러닝의 큰 범주의 개념들과 흐름을 파악할 수 있었다. 영어에 거부감이 있거나, 공부할 시간이 얼마 없는 사람, 그리고 딥러닝의 알고리즘부터 차근차근 공부하고 싶은 사람들에겐 추천하지 않는다. Experience-이제영처음 딥러닝을 접하는 사람에게는 추천컴퓨터 비전 기초 부분을 이해하고 싶으면 추천코딩은 약하지만 인공지능은 해보고 싶다면 추천DIGITS에 관심 있는 사람들에게는 추천Caffe를 깊게 배워 보고 싶으면 비 추천파이썬 코드를 깊게 다루면서 인공지능을 배우고 싶으면 비추천 Experience-박경훈모든 글과 강의가 영어로 되어있기 때문에 이해하기 어려울 수 있다.프로그래밍을 경험해본 사람이라면 강의내용 그대로 진행하여도 이해할 수 있어 쉽다.인증서긴 글 읽어주셔서 감사합니다!모든 리뷰는 주관적이라는점 알아주시면 감사하겠습니다!앞으로 저희의 리뷰를 통해 Nvidia에서 열리는 DLI 프로그램에 대해 궁금하신점을 해결해가셨으면 좋겠습니다!지금 리뷰는 내용 요약이 들어가있지만, 실제로 DLI프로그램을 진행하면 직접 코딩하시면서 하실 수 있습니다.이런 흐름이다~ 라고 생각하시고 보시면 좋을 것 같습니다 ㅎㅎ 감사합니다!다음주에는 CUDA C/C++리뷰로 돌아오겠습니다!"
    } 
  
]
